import { NextRequest, NextResponse } from "next/server";
import { Agent, Runner, withTrace } from "@openai/agents";
import pdf from "pdf-parse";

export const runtime = "nodejs";
export const dynamic = "force-dynamic";

/** ========= PDF â†’ Text  ========= */
async function pdfToText(file: File): Promise<string> {
  const buf = Buffer.from(await file.arrayBuffer());
  const data = await pdf(buf).catch(() => null);
  if (!data || !data.text?.trim()) throw new Error("PDF-Text leer oder nicht lesbar.");
  // leichte Normalisierung
  let text = data.text
    .replace(/\u0000/g, "")
    .replace(/\r/g, "\n")
    .replace(/[ \t]+/g, " ")
    .replace(/\n{3,}/g, "\n\n")
    .trim();
  return text;
}

/** ========= Token-SchÃ¤tzung (grob) =========
 * GPT-5/4.1 grob ~ 4 Zeichen pro Token. Wir arbeiten mit Zeichenlimits. */
function estimateTokens(s: string): number {
  return Math.ceil(s.length / 4);
}

/** ========= Semantischer Chunker =========
 * - Split nach starken Grenzen (Ãœberschriften, Artikel/Paragraphen, Anhang)
 * - Dann nach AbsÃ¤tzen
 * - Chunk-Zusammenbau bis targetTokens, Hard-Cap bei hardMaxTokens
 */
function semanticChunkText(
  text: string,
  targetTokens = 8_000,   // â‰ˆ 32k Zeichen
  hardMaxTokens = 9_500   // â‰ˆ 38k Zeichen, knapp unterm 10k-Block
): string[] {
  const strongDelim = new RegExp(
    [
      // Deutsche Ãœberschriften/Strukturmarker
      String.raw`(?=^.{0,6}(?:Kapitel|Abschnitt|Artikel|Art\.|Â§|Ziffer|Anhang)\b)`,
      // Nummerierte Hauptpunkte
      String.raw`(?=^\s*(?:[IVXLC]+\.)\s)`,
      String.raw`(?=^\s*(?:\d{1,2}\.)\s)`,
      // fette/trennzeilenartige Ãœberschriften
      String.raw`(?=^\s*[A-ZÃ„Ã–Ãœ][A-ZÃ„Ã–Ãœ \-/]{5,}\s*$)`,
    ].join("|"),
    "m"
  );

  // 1) Vorsegmentierung an starken Grenzen
  let blocks = text.split(strongDelim).map(s => s.trim()).filter(Boolean);

  // Fallback: wenn kaum starke Grenzen gefunden â†’ groÃŸe BlÃ¶cke = gesamter Text
  if (blocks.length <= 1) {
    blocks = text.split(/\n{2,}/).map(s => s.trim()).filter(Boolean);
  }

  // 2) Feiner: GroÃŸe BlÃ¶cke noch an Absatzgrenzen teilen
  const refined: string[] = [];
  for (const b of blocks) {
    if (estimateTokens(b) <= hardMaxTokens) {
      refined.push(b);
    } else {
      // Absatzweise splitten
      const paras = b.split(/\n{2,}/).map(s => s.trim()).filter(Boolean);
      let buf: string[] = [];
      let bufTokens = 0;
      for (const p of paras) {
        const t = estimateTokens(p) + 2; // +2 fÃ¼r Absatztrenner
        if (bufTokens + t > targetTokens && buf.length) {
          refined.push(buf.join("\n\n"));
          buf = [p];
          bufTokens = estimateTokens(p);
        } else if (t > hardMaxTokens) {
          // Extrem langer Absatz â†’ hart schneiden
          const chunks = hardSplitByChars(p, hardMaxTokens * 4); // *4 â†’ Zeichen
          refined.push(...chunks);
          buf = [];
          bufTokens = 0;
        } else {
          buf.push(p);
          bufTokens += t;
        }
      }
      if (buf.length) refined.push(buf.join("\n\n"));
    }
  }

  // 3) Merge benachbarter Mini-BlÃ¶cke (zu klein)
  const MIN_TOK = 1_000;
  const merged: string[] = [];
  let cursor = "";
  let curTok = 0;
  for (const seg of refined) {
    const t = estimateTokens(seg);
    if (!cursor) {
      cursor = seg;
      curTok = t;
      continue;
    }
    if (curTok < MIN_TOK && curTok + t <= hardMaxTokens) {
      cursor = cursor + "\n\n" + seg;
      curTok += t;
    } else {
      merged.push(cursor);
      cursor = seg;
      curTok = t;
    }
  }
  if (cursor) merged.push(cursor);

  return merged;
}

/** Hartes Zeichen-Splitting falls ein Absatz zu groÃŸ ist */
function hardSplitByChars(s: string, maxChars: number): string[] {
  const out: string[] = [];
  for (let i = 0; i < s.length; i += maxChars) {
    out.push(s.slice(i, i + maxChars));
  }
  return out;
}

/** ========= JSON aus Agent-Output extrahieren ========= */
function extractJson(output: string): any {
  if (!output) throw new Error("Leere Antwort vom Agent.");
  const cleaned = output.replace(/```(?:json)?/gi, "").replace(/```/g, "").trim();

  // Finde den grÃ¶ÃŸten JSON-Block im Text
  const match = cleaned.match(/\{[\s\S]*\}/);
  if (match) {
    try {
      return JSON.parse(match[0]);
    } catch (err) {
      console.warn("JSON-Parsing-Warnung:", err);
    }
  }
  throw new Error("Konnte keinen gÃ¼ltigen JSON-Block aus der Agent-Antwort extrahieren.");
}

/** ========= Haupt-Agent ========= */
const avvCheckAgent = new Agent({
  name: "AVV-Check-Agent",
  instructions: `Rolle
Du bist ein AVV-PrÃ¼fassistent. Du prÃ¼fst AuftragsverarbeitungsvertrÃ¤ge (AVV/DPA) auf DSGVO-KonformitÃ¤t gemÃ¤ÃŸ Art. 28 Abs. 3 DSGVO und erzeugst eine strukturierte JSON-Ausgabe mit Compliance-, Risiko- und MaÃŸnahmenbewertung.

---

Eingabe und Arbeitsweise
Du erhÃ¤ltst den vollstÃ¤ndigen Vertragsinhalt (ggf. inkl. Anlagen) als Text.  
Wenn der Vertrag sehr lang ist, arbeite abschnittsweise (Chunking / map-reduce):

**Chunk-Analyse:**  
   Verarbeite 1â€“3 Seiten oder ca. 1500â€“2500 WÃ¶rter je Abschnitt.  
   Extrahiere nur relevante Kernbefunde (Art. 28-Themen + Zusatzklauseln).  
   Komprimiere sofort in Stichpunkte und Belegobjekte, keine VolltextabsÃ¤tze.  

**Zwischenspeicher (ACCUMULATOR):**  
   Nach jedem Chunk nur prÃ¤gnante EintrÃ¤ge speichern (Kategorie, Status, Zitat â‰¤ 240 Zeichen, Seitenzahl). Rohtext anschlieÃŸend verwerfen.  

**Merge-Schritt:**  
   Vereinige Chunk-Ergebnisse, dedupliziere Ã¤hnliche Findings und wÃ¤hle die stÃ¤rksten Belege.  
   Status-Entscheidung nach StÃ¤rke der Belege (met > partial > missing).  

**Finalisierung:**  
   Erstelle eine kompakte JSON-Ausgabe mit einheitlichen Statuswerten, Scoring und Handlungsempfehlungen.

Wenn File Search aktiviert ist, lade und verwende Dokumentpassagen aus dem Vector Store, anstatt den gesamten Text einzulesen.  
Analysiere nur relevante Chunks (max. 8 pro Lauf).  
Jeder Chunk wird wie eine Mini-Analyse behandelt (Status + Evidence).  
Kombiniere die Teilbefunde am Ende zu einem Gesamt-JSON gemÃ¤ÃŸ Schema.

---

Status-Mapping (Bewertungsraster)
met = â€žerfÃ¼lltâ€œ â†’ klare, ausdrÃ¼ckliche, konkrete Regelung ohne LÃ¼cke.  
partial = â€žteilweiseâ€œ â†’ vorhanden, aber vage oder ohne Fristen / Verfahren.  
missing = â€žfehltâ€œ â†’ nicht geregelt oder nur indirekt.  
present = â€žvorhandenâ€œ â†’ Zusatzklausel existiert, QualitÃ¤t unklar.  
not_found = â€žnicht gefundenâ€œ â†’ keine ErwÃ¤hnung.

---

Zu prÃ¼fende Punkte

**Art. 28 Abs. 3 DSGVO (Kern):**
â€¢ instructions_only (nur auf dokumentierte Weisung)  
â€¢ confidentiality (Vertraulichkeit)  
â€¢ security_TOMs (Technisch-organisatorische MaÃŸnahmen)  
â€¢ subprocessors (Unterauftragsverarbeiter, Zustimmung/Info)  
â€¢ data_subject_rights_support (UnterstÃ¼tzung Betroffenenrechte)  
â€¢ breach_support (UnterstÃ¼tzung Meldepflichten Art. 33/34)  
â€¢ deletion_return (LÃ¶schung / RÃ¼ckgabe nach Vertragsende)  
â€¢ audit_rights (Nachweise / Audits)

**Zusatzklauseln:**
â€¢ international_transfers (SCC / Transfermechanismen)  
â€¢ liability_cap (Haftungsbegrenzung)  
â€¢ jurisdiction (Gerichtsstand / Rechtswahl)

---

Belege (Evidence)
Maximal 2 Belege pro Kategorie.

Felder:
â€¢ quote = prÃ¤gnant, max. 240 Zeichen, keine ZeilenumbrÃ¼che  
â€¢ page = Seitenzahl (wenn bekannt)  

Nur aussagekrÃ¤ftige Passagen nutzen (z. B. Fristen, Pflichten, Verfahren).

---

Scoring (Compliance und Risiko)

**Gewichtete Compliance (0â€“100, hÃ¶her = besser):**
instructions_only 15 %, confidentiality 10 %, security_TOMs 20 %, subprocessors 15 %,  
data_subject_rights_support 10 %, breach_support 10 %, deletion_return 10 %, audit_rights 10 %.

**Punkte:** met = 1.0, partial = 0.5, missing = 0.

**Zusatz-Bonus (max +10, Deckel bei 100):**
international_transfers: present +3, met +5, partial +2.  
liability_cap: present oder met +2.  
jurisdiction: present oder met +2.

**Bewertungs-Korrekturregeln (Kalibrierung):**
- Wenn mindestens 3 Issues mit severity â‰¥ "medium" â†’ âˆ’5 Punkte vom Compliance-Score.  
- Wenn mindestens 1 Issue mit severity = "high" â†’ zusÃ¤tzlich âˆ’5 Punkte.  
- Wenn liability_cap = "missing" oder "not_found" â†’ âˆ’5 Punkte.  
- Wenn international_transfers = "missing" â†’ âˆ’3 Punkte.

**Formeln:**
compliance_score.overall = round(Î£(Gewicht Ã— Punkte) Ã— 100) + Bonus âˆ’ Korrekturen (max 100, min 0).  
risk_score.overall = 100 âˆ’ compliance_score.overall.  
risk_score.rationale = kurze deutschsprachige BegrÃ¼ndung (2â€“4 SÃ¤tze) mit Fokus auf wesentliche Risiken und LÃ¼cken.

**Bewertungskompass (Interpretation):**
Compliance â‰¥ 85 â†’ sehr gut (niedriges Risiko)  
70â€“84 â†’ solide, kleinere LÃ¼cken  
50â€“69 â†’ kritisch, mehrere SchwÃ¤chen  
< 50 â†’ unzureichend, hohes Risiko

---

Chunking-Strategie (Token-optimiert)
Ein Chunk â‰ˆ 1â€“3 Seiten oder â‰¤ 2500 WÃ¶rter.  
Nach jedem Chunk: Befunde extrahieren â†’ komprimieren â†’ Rohtext lÃ¶schen.  
Bei sehr langen VertrÃ¤gen: weniger Details, keine Vollzitate auÃŸer Belegen.  
Wenn Token-Limit naht: komprimieren statt abbrechen.

---

Ausgabeformat

Antworte ausschlieÃŸlich mit **einem einzigen JSON-Objekt**, keinem FlieÃŸtext auÃŸerhalb.  

FÃ¼ge am Anfang das Feld "executive_summary" hinzu (max. 8 Zeilen, deutsch, kein Marketingtext).

Danach folgen alle Felder gemÃ¤ÃŸ response_schema.

**Format-Regeln (hart):**
- Nur zulÃ¤ssige Statuswerte nutzen:  
  â€¢ Art. 28: "met" | "partial" | "missing"  
  â€¢ additional_clauses: "present" | "met" | "partial" | "missing" | "not_found"  
- Evidence:  
  â€¢ quote Pflicht (max 240 Zeichen, keine ZeilenumbrÃ¼che)  
  â€¢ page nur wenn bekannt (als Ganzzahl)  
  â€¢ niemals page =null/""  
- Actions: severity = "high" | "medium" | "low"  
- Keine unquotierten Keys, keine Ã¼berflÃ¼ssigen Kommas.

---

ðŸª¶ Executive Summary (max. 8 Zeilen, deutsch)

Fasse das PrÃ¼fergebnis prÃ¤gnant und strukturiert zusammen:

Gesamteindruck â†’ DSGVO-KonformitÃ¤t & Allgemeinbewertung  
StÃ¤rken â†’ z. B. SCC-Einbindung, TOMs, Weisungs- und Auditrechte  
LÃ¼cken â†’ z. B. Fristen, Betroffenenrechte, LÃ¶schung, Haftung  
RisikoeinschÃ¤tzung â†’ niedrig / mittel / hoch  
Empfehlung â†’ konkrete VerbesserungsmaÃŸnahme in einem Satz  

Beispiel:
"Der AVV erfÃ¼llt die wesentlichen DSGVO-Pflichten (Art. 28 Abs. 3) und integriert SCC-Regelungen. TOMs und Subprozessor-Regelungen sind solide, jedoch fehlen prÃ¤zise Fristen fÃ¼r LÃ¶schung und Betroffenenrechte. Geringes Restrisiko â€“ Empfohlen: Haftungs- und Auditverfahren ergÃ¤nzen."

---

Zusatzregeln
â€¢ contract_metadata.date = ISO-Datum oder leer.  
â€¢ parties.role = Original oder normiert (controller â†” Verantwortlicher, processor â†” Auftragsverarbeiter).  
â€¢ Wenn Land nicht ermittelbar, verwende den ISO-Code des anderen Vertragspartners oder "DE".  
â€¢ Unsichere FÃ¤lle â†’ status = "partial" und BegrÃ¼ndung in risk_score.rationale vermerken.  
â€¢ Keine Meta-Kommentare, keine Redundanzen.


Wenn Benutzer Dateien hochladen, speichere und indiziere sie automatisch im Vector Store â€žavv-filesâ€œ.Â 
Verwende anschlieÃŸend die File Search API, um relevante Passagen aus diesen Dateien zu analysieren.Â 
Falls File Search keine Ergebnisse liefert, analysiere stattdessen direkt den Volltext.

Bevor du mit der Analyse beginnst, prÃ¼fe, ob der Vector Store â€žavv-filesâ€œ aktiv ist 
und ob mindestens eine Datei eingebettet ist (Size > 0). 
Wenn nicht, analysiere die neu hochgeladene Datei direkt und fÃ¼ge sie anschlieÃŸend in den Store ein.`,
  model: "gpt-5-chat-latest",
  modelSettings: { temperature: 0.2, maxTokens: 4000, topP: 1, store: false },
});


/** ========= POST ========= */
export async function POST(req: NextRequest) {
  try {
    const form = await req.formData().catch(() => null);
    let inputText = "";

    if (form) {
      const file = form.get("file") as File | null;
      const text = (form.get("text") as string | null)?.trim();
      if (file && file.type?.includes("pdf")) inputText = await pdfToText(file);
      else if (text) inputText = text;
    } else {
      const json = await req.json().catch(() => null);
      inputText = json?.text?.trim() ?? "";
    }

    if (!inputText) {
      return NextResponse.json({ error: "Kein Text Ã¼bergeben." }, { status: 400 });
    }

    // ====== Semantisches Chunking ======
    const chunks = semanticChunkText(inputText, 8_000, 9_500);

    const runner = new Runner();
    const partialResults: any[] = [];

    // ====== Chunk-Analyse mit Backoff ======
    for (let i = 0; i < chunks.length; i++) {
      const input = `Teil ${i + 1}/${chunks.length}:\n\n${chunks[i]}`;
      const res = await withTrace(`chunk-${i + 1}`, async () =>
        runWithBackoff(() =>
          runner.run(avvCheckAgent, [{ role: "user", content: [{ type: "input_text", text: input }] }])
        )
      );
      if (res?.finalOutput) {
        const parsed = extractJson(res.finalOutput);
        partialResults.push(parsed);
      }
    }

    // ====== ZusammenfÃ¼hrung ======
    const mergeInput =
      `Hier sind ${partialResults.length} JSON-Ergebnisse aus AVV-Teilanalysen.\n` +
      `Fasse sie zu einem konsistenten Gesamt-JSON im gleichen Format zusammen.\n\n` +
      JSON.stringify(partialResults, null, 2);

    const merged = await runWithBackoff(() =>
      runner.run(avvCheckAgent, [{ role: "user", content: [{ type: "input_text", text: mergeInput }] }])
    );

    if (!merged?.finalOutput) {
        throw new Error("Merge-Agent lieferte keine finale Ausgabe.");
    }
    const finalJson = extractJson(merged.finalOutput);
    return NextResponse.json(finalJson);
  } catch (e: any) {
    return NextResponse.json(
      { error: "Agent-Serverfehler", details: e?.message ?? String(e) },
      { status: 500 }
    );
  }
}

/** ========= Exponentielles Backoff fÃ¼r TPM/Ratenfehler ========= */
async function runWithBackoff<T>(fn: () => Promise<T>, maxRetries = 3): Promise<T> {
  let delay = 800;
  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (err: any) {
      const msg = String(err?.message || err);
      const isRateLimit =
        /too_many_requests|rate limit|tokens per min|tpm|overloaded/i.test(msg) ||
        err?.code === "too_many_requests";

      if (!isRateLimit || attempt === maxRetries) throw err;
      await new Promise(r => setTimeout(r, delay));
      delay *= 2; // Exponential
    }
  }
  // Unreachable
  throw new Error("Backoff failed.");
}